{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer all questions and submit them either as an IPython notebook, LaTeX document, or Markdown document. Provide full answers for each question, including interpretation of the results. Each question is worth 25 points.\n",
    "\n",
    "This homework is due on Monday, November 30, 2015."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "\n",
    "The `titanic.xls` spreadsheet in the `data` directory contains data regarding the passengers on the Titanic when it sank in 1912. A recent [Kaggle competition](http://www.kaggle.com/c/titanic-gettingStarted) was based on predicting survival for passengers based on the attributes in the passenger list. \n",
    "\n",
    "Use scikit-learn to build both a support vector classifier and a logistic regression model to predict survival on the Titanic. Use cross-validation to assess your models, and try to tune them to improve performance.\n",
    "\n",
    "Discuss the benefits and drawbacks of both approaches for application to such problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Write your work here\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn import svm\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn import preprocessing\n",
    "import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Impute Missing Values\n",
    "#Note: This is not my code. It is a code for dealing with strings and numbers simultaneously from\n",
    "# http://stackoverflow.com/questions/25239958/impute-categorical-missing-values-in-scikit-learn\n",
    "\n",
    "from sklearn.base import TransformerMixin\n",
    "\n",
    "class DataFrameImputer(TransformerMixin):\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Impute missing values.\n",
    "\n",
    "        Columns of dtype object are imputed with the most frequent value\n",
    "        in column.\n",
    "\n",
    "        Columns of other types are imputed with mean of column.\n",
    "\n",
    "        \"\"\"\n",
    "    def fit(self, X, y=None):\n",
    "\n",
    "        self.fill = pd.Series([X[c].value_counts().index[0]\n",
    "            if X[c].dtype == np.dtype('O') else X[c].mean() for c in X],\n",
    "            index=X.columns)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        return X.fillna(self.fill)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "titanic = pd.read_excel(\"../data/titanic.xls\", \"titanic\")\n",
    "# print(titanic.head())\n",
    "titanic = DataFrameImputer().fit_transform(titanic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['pclass' 'survived' 'name' 'sex' 'age' 'sibsp' 'parch' 'ticket' 'fare'\n",
      " 'cabin' 'embarked' 'boat' 'body']\n"
     ]
    }
   ],
   "source": [
    "newCols = titanic.columns.values\n",
    "newCols[-1] = 'home'\n",
    "titanic.columns = newCols\n",
    "homeDest = titanic.pop(\"home\")\n",
    "print(titanic.columns.values)\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(titanic.sex)\n",
    "titanic['sex'] = le.transform(titanic['sex'])\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "for index, row in titanic.iterrows():\n",
    "    titanic.iloc[index].cabin = str(row.cabin).replace(\" \",\"\")\n",
    "    titanic.iloc[index,7] = str(titanic.iloc[index].ticket).replace(\" \",\"\")\n",
    "    # print(row.ticket)\n",
    "    # print(titanic.iloc[index].ticket)\n",
    "\n",
    "le.fit(titanic.cabin)\n",
    "titanic['cabin'] = le.transform(titanic['cabin'])\n",
    "\n",
    "le.fit(titanic.ticket)\n",
    "titanic['ticket'] = le.transform(titanic['ticket'])\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(titanic.embarked)\n",
    "titanic['embarked'] = le.transform(titanic['embarked'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "survived = titanic.pop('survived')\n",
    "y = survived.values\n",
    "x = titanic[['pclass','sex','age','sibsp','parch','fare','body']].values\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing c = 1\n",
      "Testing c = 10\n",
      "Testing c = 100\n",
      "Testing c = 1000\n",
      "100\n",
      "0.784532467532\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>222</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>57</td>\n",
       "      <td>121</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted    0    1\n",
       "Actual             \n",
       "0          222   32\n",
       "1           57  121"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for row in x:\n",
    "    if np.isreal(x).all():\n",
    "        continue\n",
    "    else:\n",
    "        print(\"error in row\")\n",
    "        \n",
    "cVals = [1,10,100,1000]\n",
    "scoreBest = 0\n",
    "cBest = np.nan\n",
    "from sklearn import model_selection\n",
    "for c in cVals:\n",
    "    print(\"Testing c = \" + str(c))\n",
    "    svc = svm.SVC(kernel='linear',C=c)\n",
    "    scores = model_selection.cross_val_score(svc,x_train,y_train,cv=5)\n",
    "    score = sum(scores)/len(scores)\n",
    "    if score>scoreBest:\n",
    "        cBest = c\n",
    "        scoreBest = score\n",
    "\n",
    "print(cBest)\n",
    "print(scoreBest)\n",
    "svc = svm.SVC(kernel='linear',C=cBest).fit(x_train,y_train)\n",
    "\n",
    "predTest = svc.predict(x_test)\n",
    "\n",
    "pd.crosstab(y_test, predTest, rownames=[\"Actual\"], colnames=[\"Predicted\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing c = 1\n",
      "Testing c = 10\n",
      "Testing c = 100\n",
      "Testing c = 1000\n",
      "1\n",
      "0.667006493506\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>233</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>126</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted    0   1\n",
       "Actual            \n",
       "0          233  21\n",
       "1          126  52"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cVals = [1,10,100,1000]\n",
    "scoreBest = 0\n",
    "cBest = np.nan\n",
    "from sklearn import model_selection\n",
    "for c in cVals:\n",
    "    print(\"Testing c = \" + str(c))\n",
    "    svc = svm.SVC(kernel='rbf',C=c)\n",
    "    scores = model_selection.cross_val_score(svc,x_train,y_train,cv=5)\n",
    "    score = sum(scores)/len(scores)\n",
    "    if score>scoreBest:\n",
    "        cBest = c\n",
    "        scoreBest = score\n",
    "\n",
    "print(cBest)\n",
    "print(scoreBest)\n",
    "svc = svm.SVC(kernel='rbf',C=cBest).fit(x_train,y_train)\n",
    "\n",
    "predTest = svc.predict(x_test)\n",
    "\n",
    "pd.crosstab(y_test, predTest, rownames=[\"Actual\"], colnames=[\"Predicted\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 0.1}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>229</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>69</td>\n",
       "      <td>109</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted    0    1\n",
       "Actual             \n",
       "0          229   25\n",
       "1           69  109"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#do logistic regression\n",
    "from sklearn import svm, grid_search\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {'C': [1, 0.1, 0.05, 0.02, 0.01]\n",
    "            }\n",
    "clf = GridSearchCV(LogisticRegression(penalty='l2'), param_grid).fit(x_train,y_train)\n",
    "\n",
    "# best hyperparameter setting\n",
    "print(clf.best_params_)\n",
    "\n",
    "#fit best model and predict\n",
    "estimator = LogisticRegression(penalty='l2',C=clf.best_params_['C']).fit(x_train,y_train)\n",
    "predTest = estimator.predict(x_test)\n",
    "\n",
    "pd.crosstab(y_test, predTest, rownames=[\"Actual\"], colnames=[\"Predicted\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 1}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>229</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>62</td>\n",
       "      <td>116</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted    0    1\n",
       "Actual             \n",
       "0          229   25\n",
       "1           62  116"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid = {'C': [10,1, 0.1, 0.05, 0.02, 0.01]\n",
    "            }\n",
    "clf = GridSearchCV(LogisticRegression(penalty='l1'), param_grid).fit(x_train,y_train)\n",
    "\n",
    "# best hyperparameter setting\n",
    "print(clf.best_params_)\n",
    "\n",
    "#fit best model and predict\n",
    "estimator = LogisticRegression(penalty='l2',C=clf.best_params_['C']).fit(x_train,y_train)\n",
    "predTest = estimator.predict(x_test)\n",
    "\n",
    "pd.crosstab(y_test, predTest, rownames=[\"Actual\"], colnames=[\"Predicted\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the results that we have, both SVM and logistic regression do a good job in predicting survival based on the covariates. SVMs outperform logistic regression slightly with lesser false positives as well as false negatives. The advantage of SVM is that it can come up with decision boundaries that maximize the classification margin. Another obvious advantage of SVMs is the provision to use the \"kernel trick\". However, kernel tricks are not exclusive to SVMs and can be used for logistic regression as well. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "\n",
    "The data in `prostate.data.txt` come from a study by Stamey et al. (1989), which examined the correlation between the level of prostate-specific antigen (`lpsa`) and a number of clinical measures in men who were about to receive a radical prostatectomy. The variables are log cancer volume (`lcavol`), log prostate weight (`lweight`), age, log of the amount of benign prostatic hyperplasia (`lbph`), seminal vesicle invasion (`svi`), log of capsular penetration (`lcp`), Gleason score (`gleason`), and percent of Gleason scores 4 or 5 (`pgg45`). \n",
    "\n",
    "1. Select (your choice) five competing 3-variable linear regression models, and compare them using AIC, five-fold and ten-fold cross-validation. Discuss the results.\n",
    "\n",
    "2. An alternative method for model assessment is to fit the models on a set of bootstrap samples, and then keep track of how well it predicts the original training set. If $\\hat{f}^b(x_i)$ is the predicted value at $x_i$, from the model fitted to the bth bootstrap dataset, such an estimate is:\n",
    "$$\\frac{1}{B} \\frac{1}{N} \\sum_{b=1}^B \\sum_{i=1}^N L(y_i,\\hat{f}^b(x_i)) $$\n",
    "However, because the bootstrap samples tend to contain many observations in common among the set of bootstrap samples, this estimate will tend to underestimate the true error rate. The so-called .632 estimator aleviates this bias by returning a weighted average of the training error (average loss over the training sample) and the leave-one-out (LOO) bootstrap error:\n",
    "$$\\hat{err}^{(.632)} = 0.368 \\, \\bar{err} + 0.632 \\, \\hat{err}^{(1)}$$\n",
    "where:\n",
    "$$\\bar{err} = \\frac{1}{N}\\sum_{i=1}^N L(y_i, \\hat{f}(x_i)) $$\n",
    "Repeat the assesment from part (1) using the .632 estimator, and compare the result to the other approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Write your work here\n",
    "#read the data\n",
    "data = pd.read_table(\"../data/prostate.data.txt\")\n",
    "data.replace({'train':{'T':1,'F':0}},inplace=True)\n",
    "data.drop(data.columns[0],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "aic = lambda n,rss,numParams: n + n*np.log(2*3.14) + n*np.log(rss/n) + 2*(numParams+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#create x and y values\n",
    "y = data.pop('lpsa').values\n",
    "mask = np.ones(len(data.columns.values),dtype=bool)\n",
    "mask[-2] = 0\n",
    "XAll = data[data.columns[mask]]\n",
    "numColumns = len(XAll.columns.values)\n",
    "n = len(data.index)#size of the data\n",
    "n\n",
    "aicCurr = aic(1,1,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Predictors by 10 Round Cross Validation are: \n",
      "['lweight' 'train' 'lcavol']\n",
      "Error in best prediction is: \n",
      "0.912725585571\n",
      "Best Predictors by 5 Round Cross Validation are: \n",
      "['lweight' 'train' 'lcavol']\n",
      "Error in best prediction is: \n",
      "0.734461018736\n",
      "Best Predictors by AIC are: \n",
      "['lweight' 'train' 'lcavol']\n",
      "Error in best prediction is: \n",
      "0.912725585571\n"
     ]
    }
   ],
   "source": [
    "selectedCovariates = np.zeros(5,dtype=object) #keep record of the performances of the covariates\n",
    "\n",
    "numCompetitors = 5\n",
    "# cvVals=10\n",
    "#check variables for the loop\n",
    "bestError10, bestIndex10, bestError5, bestIndex5, bestAIC, bestIndexAIC = 1e10,0,1e10,0,1e10,0\n",
    "predictors = []\n",
    "\n",
    "for roundPrediction in range(numCompetitors):\n",
    "    #randomly select three columns\n",
    "    choice = np.random.choice(numColumns,3,replace=False)\n",
    "    predictors.append(choice)\n",
    "    mask = np.zeros(len(data.columns.values),dtype=bool)\n",
    "    mask[choice] = 1\n",
    "\n",
    "\n",
    "    #get the selected covariates\n",
    "    xCurr = XAll[XAll.columns[mask]].values\n",
    "\n",
    "    from sklearn import linear_model, model_selection\n",
    "    reg = linear_model.LinearRegression()\n",
    "\n",
    "    #fit the object\n",
    "    error10 = (sum(abs((model_selection.cross_val_score(reg,xCurr,y,cv=10,scoring='mean_squared_error'))))/10)\n",
    "    error5 = (sum(abs((model_selection.cross_val_score(reg,xCurr,y,cv=5,scoring='mean_squared_error'))))/5)\n",
    "    #calculte AIC\n",
    "    reg.fit(xCurr,y)\n",
    "    err = (np.sum((reg.predict(xCurr)-y)**2))\n",
    "    aicCurr = aic(n,err,4)\n",
    "\n",
    "    if error10 < bestError10:\n",
    "        bestError10 = error10\n",
    "        bestIndex10 = roundPrediction\n",
    "    if error5 < bestError5:\n",
    "        bestError5 = error5\n",
    "        bestIndex5 = roundPrediction\n",
    "    if aicCurr < bestAIC:\n",
    "        bestAIC = aicCurr\n",
    "        bestIndexAIC = roundPrediction\n",
    "        \n",
    "print(\"Best Predictors by 10 Round Cross Validation are: \")\n",
    "print(XAll[XAll.columns[predictors[bestIndex10]]].columns.values)\n",
    "print(\"Error in best prediction is: \")\n",
    "print(bestError5)\n",
    "\n",
    "print(\"Best Predictors by 5 Round Cross Validation are: \")\n",
    "print(XAll[XAll.columns[predictors[bestIndex5]]].columns.values)\n",
    "print(\"Error in best prediction is: \")\n",
    "print(bestError10)\n",
    "\n",
    "print(\"Best Predictors by AIC are: \")\n",
    "print(XAll[XAll.columns[predictors[bestIndexAIC]]].columns.values)\n",
    "print(\"Error in best prediction is: \")\n",
    "print(error5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Predictors are: \n",
      "['lbph' 'lcavol' 'gleason']\n",
      "Error in best prediction is: \n",
      "0.580403800606\n"
     ]
    }
   ],
   "source": [
    "#PART 2: bootstrap sample\n",
    "from sklearn.utils import resample\n",
    "#had dropped y column. re-read. Not the best idea\n",
    "data = pd.read_table(\"../data/prostate.data.txt\")\n",
    "data.replace({'train':{'T':1,'F':0}},inplace=True)\n",
    "\n",
    "dataCopy = data.copy(deep=True)\n",
    "\n",
    "data.drop(data.columns[0],axis=1,inplace=True)\n",
    "\n",
    "\n",
    "numCompetitors = 5\n",
    "cvVals=10\n",
    "bestError, bestIndex = 1e10,0\n",
    "predictors = []\n",
    "\n",
    "bootstrapRounds = 5\n",
    "\n",
    "for roundPrediction in range(numCompetitors):\n",
    "#   print(data.columns.values)\n",
    "    choice = np.random.choice(numColumns,3,replace=False)\n",
    "    errorTraining = 0\n",
    "    errorLOO = 0\n",
    "    for roundBootStrap in range(bootstrapRounds):\n",
    "        dataCopy = data.copy(deep=True)\n",
    "        #mask to select the x covariates\n",
    "        mask = np.ones(len(data.columns.values)-1,dtype=bool)#will get rid of y variable\n",
    "        mask[-2] = 0\n",
    "\n",
    "        dataBootstrap = resample(data)\n",
    "        y = dataBootstrap.pop('lpsa').values\n",
    "        XAll = dataBootstrap[dataBootstrap.columns[mask]]\n",
    "\n",
    "        #randomly pick three convariates\n",
    "\n",
    "        predictors.append(choice)\n",
    "        #mask to select the three chosen covariates\n",
    "        mask = np.zeros(len(XAll.columns.values),dtype=bool)\n",
    "        mask[choice] = 1\n",
    "        xCurr = XAll[XAll.columns[mask]].values\n",
    "        reg.fit(xCurr,y)\n",
    "\n",
    "        #calculate mean squared error on total data\n",
    "        errorTrainingCurr = sum((reg.predict(data[data.columns[mask]].values) - dataCopy.pop('lpsa').values) ** 2)/len(data.index)\n",
    "\n",
    "        #do LOO\n",
    "        from sklearn import cross_validation\n",
    "        loo = cross_validation.LeaveOneOut(len(data.index))\n",
    "        errorLOOCurr = sum(abs(model_selection.cross_val_score(reg,xCurr,y,scoring='mean_squared_error',cv=loo,)))/len(data.index)\n",
    "        \n",
    "        errorTraining+=errorTrainingCurr\n",
    "        errorLOO+=errorLOOCurr\n",
    "        \n",
    "    totalError = 0.368*errorTraining + 0.632*errorLOO\n",
    "\n",
    "    if totalError<bestError:\n",
    "        bestError = totalError\n",
    "        bestIndex = roundPrediction\n",
    "\n",
    "        #print(data.columns.values)\n",
    "\n",
    "\n",
    "print(\"Best Predictors are: \")\n",
    "print(XAll[XAll.columns[predictors[bestIndex]]].columns.values)\n",
    "print(\"Error in best prediction is: \")\n",
    "print(bestError/5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that .632 estimator performs better than finding the best model using Cross-Validation and the AIC models. However, this result varies and of the many runs that I tried, Cross-Validation and AIC often came out on top. Standard bootstrapping, often produced a bias (though reduced the variance) but it was corrected by using the 0.632 rule."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3\n",
    "\n",
    "Fit a series of random-forest classifiers to the very low birthweight infant data (`vlbw.csv`), to explore the sensitivity to the parameter `m`, the number of variables considered for splitting at each step. Plot both the out-of-bag error as well as the test error against a suitably-chosen range of values for `m`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['birth', 'exit', 'hospstay', 'lowph', 'pltct', 'race', 'bwt', 'gest', 'inout', 'twn', 'lol', 'magsulf', 'meth', 'toc', 'delivery', 'apg1', 'vent', 'pneumo', 'pda', 'cld', 'pvh', 'ivh', 'ipe', 'year', 'sex']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x10cf7bc10>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXsAAAEACAYAAABS29YJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10bae4250>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Write your work here\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.base import TransformerMixin\n",
    "\n",
    "class DataFrameImputer(TransformerMixin):\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Impute missing values.\n",
    "\n",
    "        Columns of dtype object are imputed with the most frequent value\n",
    "        in column.\n",
    "\n",
    "        Columns of other types are imputed with mean of column.\n",
    "\n",
    "        \"\"\"\n",
    "    def fit(self, X, y=None):\n",
    "\n",
    "        self.fill = pd.Series([X[c].value_counts().index[0]\n",
    "            if X[c].dtype == np.dtype('O') else X[c].mean() for c in X],\n",
    "            index=X.columns)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        return X.fillna(self.fill)\n",
    "\n",
    "\n",
    "#read data\n",
    "data = pd.read_csv(\"../data/vlbw.csv\")\n",
    "data.drop(data.columns[0],axis=1)\n",
    "columnNames = list(data.columns.values)\n",
    "data = data.ix[:,1:len(columnNames)]\n",
    "# print(data.columns.values)\n",
    "#print(data.head())\n",
    "data = DataFrameImputer().fit_transform(data)\n",
    "\n",
    "columnNames = list(data.columns.values)\n",
    "xColumns = [col for col in columnNames if col != \"dead\"]\n",
    "print(xColumns)\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "\n",
    "for column in columnNames:\n",
    "    le.fit(data[column])\n",
    "    data[column] = le.transform(data[column])\n",
    "\n",
    "\n",
    "y = data.pop(\"dead\").values\n",
    "x = data[xColumns].values\n",
    "maxEstimators = len(xColumns)\n",
    "# print(maxEstimators)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.33, random_state=42)\n",
    "\n",
    "oobErrors = []\n",
    "testErrors = []\n",
    "\n",
    "for counterEstimator in range(1,maxEstimators+1):\n",
    "    randForestClass = RandomForestClassifier(n_estimators=100,oob_score=True,max_features=counterEstimator)\n",
    "    randForestClass.fit(x_train,y_train)\n",
    "    #calculate test Error\n",
    "    yPred = randForestClass.predict(x_test)\n",
    "    #0s and 1s. Take NOT(XOR) to find non matches. \n",
    "    testResults = np.logical_not(np.logical_xor(yPred,y_test))\n",
    "    testError = (len(testResults) - np.sum(testResults))/len(testResults)\n",
    "    testErrors.append(testError)\n",
    "    #calculate OOB Error\n",
    "    oobError = 1 - randForestClass.oob_score_\n",
    "#     print(oobError)\n",
    "    oobErrors.append(oobError)\n",
    "\n",
    "plt.plot(range(maxEstimators),oobErrors,label='OOB Error')\n",
    "plt.plot(range(maxEstimators),testErrors,label='Test Error')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the OOB error is more than the Test Error. The reason for this, as mentioned in Efron and Tibshirani, (1993), is that \"the bootstrap samples used to compute the [OOB error rate] are further away on the average than a typical test sample\". Therefore the OOB error estimates are pessimistic in nature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus: Question 4\n",
    "\n",
    "Use a grid search to optimize the number of estimators and max_depth for a Gradient Boosted Decision tree using the very low birthweight infant data. Plug this optimal ``max_depth`` into a *single* decision tree.  Does this single tree over-fit or under-fit the data? Repeat this for the Random Forest.  Construct a single decision tree using the ``max_depth`` which is optimal for the Random Forest.  Does this single tree over-fit or under-fit the data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Write your work here\n",
    "__author__ = 'ayanmukhopadhyay'\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.base import TransformerMixin\n",
    "\n",
    "class DataFrameImputer(TransformerMixin):\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Impute missing values.\n",
    "\n",
    "        Columns of dtype object are imputed with the most frequent value\n",
    "        in column.\n",
    "\n",
    "        Columns of other types are imputed with mean of column.\n",
    "\n",
    "        \"\"\"\n",
    "    def fit(self, X, y=None):\n",
    "\n",
    "        self.fill = pd.Series([X[c].value_counts().index[0]\n",
    "            if X[c].dtype == np.dtype('O') else X[c].mean() for c in X],\n",
    "            index=X.columns)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        return X.fillna(self.fill)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#read data\n",
    "data = pd.read_csv(\"../data/vlbw.csv\")\n",
    "# print(data.columns.values)\n",
    "data.drop(data.columns[0],axis=1,inplace=True)\n",
    "data = DataFrameImputer().fit_transform(data)\n",
    "\n",
    "columnNames = list(data.columns.values)\n",
    "xColumns = [col for col in columnNames if col != \"dead\"]\n",
    "# print(xColumns)\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "\n",
    "for column in columnNames:\n",
    "    le.fit(data[column])\n",
    "    data[column] = le.transform(data[column])\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "y = data.pop(\"dead\").values\n",
    "x = data[xColumns].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.33)\n",
    "maxEstimators = len(xColumns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': 3, 'n_estimators': 30}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "param_grid = {'max_depth': list(range(1,20)),\n",
    "              'n_estimators' : [5,10,20,30]\n",
    "              }\n",
    "\n",
    "est = GradientBoostingClassifier(n_estimators=3000)\n",
    "gs_cv = GridSearchCV(est, param_grid, n_jobs=4).fit(X_train, y_train)\n",
    "\n",
    "# best hyperparameter setting\n",
    "gs_cv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Score\n",
      "0.887387387387\n",
      "Training Score\n",
      "0.86859688196\n"
     ]
    }
   ],
   "source": [
    "#now fit this max depth to a single decision tree\n",
    "from sklearn import tree\n",
    "clf = tree.DecisionTreeClassifier(criterion='entropy',max_features=\"auto\",max_depth=gs_cv.best_params_[\"max_depth\"])\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "print(\"Test Score\")\n",
    "print(clf.score(X_test,y_test))\n",
    "print(\"Training Score\")\n",
    "print(clf.score(X_train, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 500, 'max_depth': 9}\n",
      "Test Score\n",
      "0.887387387387\n",
      "Training Score\n",
      "0.993318485523\n"
     ]
    }
   ],
   "source": [
    "param_grid = {'max_depth': list(range(1,30)),\n",
    "#               'min_samples_leaf': [3, 5, 9, 17],\n",
    "              'n_estimators': [10,100,500,1000,2000,3000]\n",
    "              }\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "est = RandomForestClassifier(n_estimators=3000)\n",
    "# this may take some minutes\n",
    "gs_cv = GridSearchCV(est, param_grid, n_jobs=4).fit(X_train, y_train)\n",
    "\n",
    "# best hyperparameter setting\n",
    "print(gs_cv.best_params_)\n",
    "\n",
    "#now fit this max depth to a single decision tree\n",
    "from sklearn import tree\n",
    "clf = tree.DecisionTreeClassifier(criterion='entropy',max_features=\"auto\",max_depth=gs_cv.best_params_[\"max_depth\"])\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "print(\"Test Score\")\n",
    "print(clf.score(X_test,y_test))\n",
    "print(\"Training Score\")\n",
    "print(clf.score(X_train, y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to check which of the two processes \"overfit\", I checked how well the methods fit to the training data used to learn the models and the difference between the test accuracy and the training accuracy. The second method, where a random forest classifier is learnt overfits the data used. However, when used on a test set, their performances are very similar."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.3.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
